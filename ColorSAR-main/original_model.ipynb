{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JwtXHtB-vw1G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9s5wm77EW0w",
    "outputId": "9d65b29a-71ca-4196-c7d6-3a964128b8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\shash\\anaconda3\\lib\\site-packages (2.7.0+cu118)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: kaggle in c:\\users\\shash\\anaconda3\\lib\\site-packages (1.7.4.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: bleach in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (3.7)\n",
      "Requirement already satisfied: protobuf in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (4.25.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (75.1.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: webencodings in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\shash\\anaconda3\\lib\\site-packages (from bleach->kaggle) (24.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shash\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Requirement already satisfied: torchvision in c:\\users\\shash\\anaconda3\\lib\\site-packages (0.22.0+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.7.0+cu118 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torchvision) (2.7.0+cu118)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.0+cu118->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from jinja2->torch==2.7.0+cu118->torchvision) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-image in c:\\users\\shash\\anaconda3\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.11.4 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (1.13.1)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (3.3)\n",
      "Requirement already satisfied: pillow>=10.1 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (10.4.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\shash\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install kaggle\n",
    "%pip install torchvision\n",
    "%pip install scikit-image\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z_xvYy58KD0"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1L0oDgbb_27",
    "outputId": "991d6e84-9b34-49fa-d1ce-429718b13aca"
   },
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d requiemonk/sentinel12-image-pairs-segregated-by-terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FeK8RHgUcEtp"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !unzip sentinel12-image-pairs-segregated-by-terrain.zip\n",
    "# !rm -rf sentinel12-image-pairs-segregated-by-terrain.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1IETAeEcZPJ",
    "outputId": "8b758b94-2645-4c49-a5c6-bb0e7d14701c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 16000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "opt = []\n",
    "sar = []\n",
    "root_dir = '../v_2'\n",
    "for dir in os.listdir(root_dir):\n",
    "  path = os.path.join(root_dir, dir)\n",
    "  s1, s2  = os.listdir(path)\n",
    "  if s1 == 's2':\n",
    "    print(s1)\n",
    "    for file in os.listdir(os.path.join(path, s1)):\n",
    "      if file.endswith('.png'):\n",
    "        opt.append(os.path.join(path, s1, file))\n",
    "    for file in os.listdir(os.path.join(path, s2)):\n",
    "      if file.endswith('.png'):\n",
    "        sar.append(os.path.join(path, s2, file))\n",
    "  if s1 == 's1':\n",
    "    for file in os.listdir(os.path.join(path, s1)):\n",
    "      if file.endswith('.png'):\n",
    "        sar.append(os.path.join(path, s1, file))\n",
    "    for file in os.listdir(os.path.join(path, s2)):\n",
    "      if file.endswith('.png'):\n",
    "        opt.append(os.path.join(path, s2, file))\n",
    "opt = sorted(opt)\n",
    "sar = sorted(sar)\n",
    "print(len(opt), len(sar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBMA-tmHswmS"
   },
   "source": [
    "# **Implementing Colorization Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zquuhq-aCtvp"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL8MLdko5vp0"
   },
   "source": [
    "## Preparing dataset for colorization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "K_cdF36mCylJ"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def rgb_to_lab_cv2(pil_img):\n",
    "    img_np = np.array(pil_img.convert(\"RGB\")).astype(\"float32\") / 255.0\n",
    "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "    img_lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2Lab)\n",
    "    \n",
    "    # Convert to torch tensor and normalize\n",
    "    img_lab = torch.from_numpy(img_lab.transpose(2, 0, 1)).float()  # shape [3, H, W]\n",
    "    L = (img_lab[[0], ...] / 50.0) - 1.0\n",
    "    ab = (img_lab[[1, 2], ...] - 128.0) / 128.0\n",
    "\n",
    "    return L, ab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: torch.Size([1, 256, 256]) ab: torch.Size([2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(opt[0]).resize((256, 256))\n",
    "L, ab = rgb_to_lab_cv2(img)\n",
    "print(\"L:\", L.shape, \"ab:\", ab.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zNPdY1ONCy7u"
   },
   "outputs": [],
   "source": [
    "def create_patches(img, patch_size=224):\n",
    "    patches = []\n",
    "    h, w = img.shape[1:]\n",
    "\n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            patch = img[:, i:i + patch_size, j:j + patch_size]\n",
    "            if patch.shape[1:] == (patch_size, patch_size):\n",
    "                patches.append(patch)\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vVScq3AmC2d_"
   },
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, color_dir, transform=None, patch_size=224):\n",
    "        self.color_dir = color_dir\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.color_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pil_img = Image.open(self.color_dir[idx]).convert(\"RGB\").resize((224, 224))\n",
    "        L, ab = rgb_to_lab_cv2(pil_img)\n",
    "        return L, ab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "arn55ktfC4Nh"
   },
   "outputs": [],
   "source": [
    "dataset = ColorizationDataset(opt[:1000])\n",
    "\n",
    "# Split dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FW4ZPmyIMPk_",
    "outputId": "b6e6485b-8376-4b2e-fea1-dd8e8cae6c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_patches shape: torch.Size([8, 1, 224, 224])\n",
      "ab_patches shape: torch.Size([8, 2, 224, 224])\n",
      "tensor([[[-0.5271, -0.5283, -0.5426,  ..., -0.1525, -0.1548, -0.1566],\n",
      "         [-0.6339, -0.6213, -0.5721,  ..., -0.1930, -0.1665, -0.1904],\n",
      "         [-0.6472, -0.6472, -0.6414,  ..., -0.2190, -0.1923, -0.1738],\n",
      "         ...,\n",
      "         [-0.2212, -0.1898, -0.1528,  ..., -0.3651, -0.3645, -0.3840],\n",
      "         [-0.2148, -0.1914, -0.1792,  ..., -0.3939, -0.3943, -0.3999],\n",
      "         [-0.2087, -0.2087, -0.2111,  ..., -0.4369, -0.4132, -0.4275]]])\n"
     ]
    }
   ],
   "source": [
    "# Load a batch and print the shapes of the patches\n",
    "for L_patches, ab_patches in train_loader:\n",
    "    print(f\"L_patches shape: {L_patches.shape}\")\n",
    "    print(f\"ab_patches shape: {ab_patches.shape}\")\n",
    "    print(L_patches[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgE8GU69HfOB"
   },
   "source": [
    "## Implementing the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uTQjKo4vK2ME"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights, DenseNet121_Weights\n",
    "\n",
    "class EnsembleEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleEncoder, self).__init__()\n",
    "\n",
    "        # Load pre-trained ResNet50 and DenseNet121\n",
    "        self.resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.densenet121 = models.densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
    "\n",
    "        self.resnet50 = nn.Sequential(*list(self.resnet50.children())[:-2])\n",
    "        # self.densenet121 = nn.Sequential(*list(self.densenet121.children())[:-1])\n",
    "        self.densenet121.classifier = nn.Identity()\n",
    "\n",
    "\n",
    "        # Custom layers for fusion\n",
    "        self.conv1x1_resnet50 = nn.ModuleList([\n",
    "            nn.Conv2d(256, 128, kernel_size=1),\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.Conv2d(1024, 512, kernel_size=1),\n",
    "            nn.Conv2d(2048, 1024, kernel_size=1)\n",
    "        ])\n",
    "\n",
    "        self.conv1x1_densenet121 = nn.ModuleList([\n",
    "            nn.Conv2d(256, 128, kernel_size=1),\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.Conv2d(1024, 512, kernel_size=1),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "        ])\n",
    "\n",
    "        self.fusion_blocks = nn.ModuleList([\n",
    "            self.fusion_block(128, 128),\n",
    "            self.fusion_block(256, 256),\n",
    "            self.fusion_block(512, 512),\n",
    "            self.fusion_block(1024, 1024)\n",
    "        ])\n",
    "\n",
    "    # Fusion block\n",
    "    def fusion_block(self, in_channels_resnet, in_channels_densenet):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels_resnet + in_channels_densenet, in_channels_resnet, kernel_size=1),\n",
    "            nn.BatchNorm2d(in_channels_resnet),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet50\n",
    "        resnet_features = []\n",
    "        resnet_input = x\n",
    "        for i, layer in enumerate(self.resnet50.children()):\n",
    "            resnet_input = layer(resnet_input)\n",
    "            if i in [4, 5, 6, 7]:  # Extract features after specific layers\n",
    "                resnet_features.append(self.conv1x1_resnet50[i-4](resnet_input))\n",
    "\n",
    "        # Forward pass through DenseNet121\n",
    "        densenet_features = []\n",
    "        idx = 0\n",
    "        densenet_input = x\n",
    "        for i, layer in enumerate(self.densenet121.features):\n",
    "            densenet_input = layer(densenet_input)\n",
    "            if i in [ 4, 6, 8, 11]:\n",
    "                densenet_features.append(self.conv1x1_densenet121[idx](densenet_input))\n",
    "                idx += 1\n",
    "\n",
    "\n",
    "        fused_features = []\n",
    "        for i in range(4):\n",
    "            fused = torch.cat((resnet_features[i], densenet_features[i]), dim=1)\n",
    "            fused = self.fusion_blocks[i](fused)\n",
    "            fused_features.append(fused)\n",
    "\n",
    "        return fused_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZAa-Gw6Jzfx"
   },
   "source": [
    "## Implementing the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mZJRvn2VHnrj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Decoder block 1: Takes input from Fusion Block 4\n",
    "        self.decode1 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 7x7 -> 14x14\n",
    "        )\n",
    "\n",
    "        # Decoder block 2: Takes input from Decoder Block 1 + Fusion Block 3 (512 + 512 channels)\n",
    "        self.decode2 = nn.Sequential(\n",
    "            nn.Conv2d(512 + 512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 14x14 -> 28x28\n",
    "        )\n",
    "\n",
    "        # Decoder block 3: Takes input from Decoder Block 2 + Fusion Block 2 (256 + 256 channels)\n",
    "        self.decode3 = nn.Sequential(\n",
    "            nn.Conv2d(256 + 256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 28x28 -> 56x56\n",
    "        )\n",
    "\n",
    "        # Decoder block 4: Takes input from Decoder Block 3 + Fusion Block 1 (128 + 128 channels)\n",
    "        self.decode4 = nn.Sequential(\n",
    "            nn.Conv2d(128 + 128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 56x56 -> 112x112\n",
    "        )\n",
    "\n",
    "        # Final decoder block: Reduce to 2 channels (ab channels)\n",
    "        self.decode5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 2, kernel_size=3, stride=1, padding=1),            \n",
    "            nn.BatchNorm2d(2),            \n",
    "            nn.Tanh(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 112x112 -> 224x224\n",
    "        )\n",
    "\n",
    "    def forward(self, features_7x7, features_14x14, features_28x28, features_56x56):\n",
    "        x = self.decode1(features_7x7)\n",
    "        x = torch.cat([x, features_14x14], dim=1)\n",
    "        x = self.decode2(x)\n",
    "\n",
    "        x = torch.cat([x, features_28x28], dim=1)\n",
    "        x = self.decode3(x)\n",
    "\n",
    "        x = torch.cat([x, features_56x56], dim=1)\n",
    "        x = self.decode4(x)\n",
    "\n",
    "        output = self.decode5(x)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNHSbR3Q6egy"
   },
   "source": [
    "## Checking our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VW-Tg5UhrVvM",
    "outputId": "a2ebc8fb-790c-4c33-e106-3c523ee72f5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ColorizationModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        features_56x56, features_28x28, features_14x14, features_7x7 = self.encoder(x)\n",
    "\n",
    "        output = self.decoder(features_7x7, features_14x14, features_28x28, features_56x56)\n",
    "\n",
    "        return output\n",
    "\n",
    "encoder = EnsembleEncoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "model = ColorizationModel(encoder, decoder)\n",
    "\n",
    "# input data\n",
    "L_patches = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "output = model(L_patches)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # output shape should be [1, 2, 224, 224]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9glT9b9vRFIC"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgmjEBEzs0aG",
    "outputId": "bd1100df-240b-47d6-9049-5b37cce38c01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 (Training): 100%|██████████████████████████████████████████████| 80/80 [00:24<00:00,  3.29it/s, loss=0.8160]\n",
      "Epoch 1/20 (Validation): 100%|████████████████████████████████████████████| 20/20 [00:05<00:00,  3.77it/s, loss=0.6993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.8160, Validation Loss: 0.6993\n",
      "Model saved with validation loss: 0.6993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 (Training): 100%|██████████████████████████████████████████████| 80/80 [00:22<00:00,  3.59it/s, loss=0.6704]\n",
      "Epoch 2/20 (Validation): 100%|████████████████████████████████████████████| 20/20 [00:05<00:00,  3.47it/s, loss=0.5257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Training Loss: 0.6704, Validation Loss: 0.5257\n",
      "Model saved with validation loss: 0.5257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 (Training): 100%|██████████████████████████████████████████████| 80/80 [00:25<00:00,  3.10it/s, loss=0.5714]\n",
      "Epoch 3/20 (Validation): 100%|████████████████████████████████████████████| 20/20 [00:06<00:00,  3.22it/s, loss=0.5470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Training Loss: 0.5714, Validation Loss: 0.5470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 (Training): 100%|██████████████████████████████████████████████| 80/80 [00:26<00:00,  3.06it/s, loss=0.4874]\n",
      "Epoch 4/20 (Validation): 100%|████████████████████████████████████████████| 20/20 [00:06<00:00,  3.28it/s, loss=0.4501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Training Loss: 0.4874, Validation Loss: 0.4501\n",
      "Model saved with validation loss: 0.4501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 (Training): 100%|██████████████████████████████████████████████| 80/80 [00:26<00:00,  3.06it/s, loss=0.4273]\n",
      "Epoch 5/20 (Validation): 100%|████████████████████████████████████████████| 20/20 [00:06<00:00,  3.26it/s, loss=0.3282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Training Loss: 0.4273, Validation Loss: 0.3282\n",
      "Model saved with validation loss: 0.3282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 (Training): 100%|██████████████████████████████████████████████| 80/80 [00:26<00:00,  3.06it/s, loss=0.3781]\n",
      "Epoch 6/20 (Validation): 100%|████████████████████████████████████████████| 20/20 [00:06<00:00,  3.27it/s, loss=0.2697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 0.3781, Validation Loss: 0.2697\n",
      "Model saved with validation loss: 0.2697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 (Training): 100%|██████████████████████████████████████████████| 80/80 [00:26<00:00,  3.05it/s, loss=0.3225]\n",
      "Epoch 7/20 (Validation): 100%|████████████████████████████████████████████| 20/20 [00:06<00:00,  3.28it/s, loss=0.2462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Training Loss: 0.3225, Validation Loss: 0.2462\n",
      "Model saved with validation loss: 0.2462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 (Training): 100%|██████████████████████████████████████████████| 80/80 [00:25<00:00,  3.09it/s, loss=0.2931]\n",
      "Epoch 8/20 (Validation): 100%|████████████████████████████████████████████| 20/20 [00:06<00:00,  3.29it/s, loss=0.2390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Training Loss: 0.2931, Validation Loss: 0.2390\n",
      "Model saved with validation loss: 0.2390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 (Training): 100%|██████████████████████████████████████████████| 80/80 [00:26<00:00,  3.08it/s, loss=0.2554]\n",
      "Epoch 9/20 (Validation): 100%|████████████████████████████████████████████| 20/20 [00:06<00:00,  3.31it/s, loss=0.1663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Training Loss: 0.2554, Validation Loss: 0.1663\n",
      "Model saved with validation loss: 0.1663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:26<00:00,  3.07it/s, loss=0.2373]\n",
      "Epoch 10/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.27it/s, loss=0.2064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Training Loss: 0.2373, Validation Loss: 0.2064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:26<00:00,  3.06it/s, loss=0.2130]\n",
      "Epoch 11/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.27it/s, loss=0.1341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Training Loss: 0.2130, Validation Loss: 0.1341\n",
      "Model saved with validation loss: 0.1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:26<00:00,  3.08it/s, loss=0.1839]\n",
      "Epoch 12/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.25it/s, loss=0.1511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Training Loss: 0.1839, Validation Loss: 0.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:26<00:00,  3.03it/s, loss=0.1649]\n",
      "Epoch 13/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.26it/s, loss=0.1541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Training Loss: 0.1649, Validation Loss: 0.1541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:25<00:00,  3.09it/s, loss=0.1732]\n",
      "Epoch 14/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.22it/s, loss=0.1063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Training Loss: 0.1732, Validation Loss: 0.1063\n",
      "Model saved with validation loss: 0.1063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:25<00:00,  3.09it/s, loss=0.1631]\n",
      "Epoch 15/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.27it/s, loss=0.0895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Training Loss: 0.1631, Validation Loss: 0.0895\n",
      "Model saved with validation loss: 0.0895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:26<00:00,  3.07it/s, loss=0.1483]\n",
      "Epoch 16/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.32it/s, loss=0.1881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Training Loss: 0.1483, Validation Loss: 0.1881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:25<00:00,  3.08it/s, loss=0.1367]\n",
      "Epoch 17/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.30it/s, loss=0.2450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Training Loss: 0.1367, Validation Loss: 0.2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:26<00:00,  3.05it/s, loss=0.1266]\n",
      "Epoch 18/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.25it/s, loss=0.0898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Training Loss: 0.1266, Validation Loss: 0.0898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:26<00:00,  3.07it/s, loss=0.1423]\n",
      "Epoch 19/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:05<00:00,  4.00it/s, loss=0.0843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Training Loss: 0.1423, Validation Loss: 0.0843\n",
      "Model saved with validation loss: 0.0843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 (Training): 100%|█████████████████████████████████████████████| 80/80 [00:26<00:00,  3.05it/s, loss=0.1162]\n",
      "Epoch 20/20 (Validation): 100%|███████████████████████████████████████████| 20/20 [00:06<00:00,  3.26it/s, loss=0.2405]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 0.1162, Validation Loss: 0.2405\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize encoder and decoder\n",
    "encoder = EnsembleEncoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "# Freeze the encoder parameters as they are pre-trained\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    encoder.eval()\n",
    "    decoder.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Training)\")\n",
    "    for i, (L_batch, ab_batch) in enumerate(train_bar):\n",
    "        L, ab = L_batch.to(device), ab_batch.to(device)\n",
    "        L = L.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        features_56x56, features_28x28, features_14x14, features_7x7 = encoder(L)\n",
    "        output = decoder(features_7x7, features_14x14, features_28x28, features_56x56)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, ab)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_bar.set_postfix(loss=f\"{running_loss/(i+1):.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    decoder.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Validation)\")\n",
    "    with torch.no_grad():\n",
    "        for i, (L_batch, ab_batch) in enumerate(val_bar):\n",
    "            L, ab = L_batch.to(device), ab_batch.to(device)\n",
    "            L = L.repeat(1, 3, 1, 1)  \n",
    "\n",
    "            # Forward pass\n",
    "            features_56x56, features_28x28, features_14x14, features_7x7 = encoder(L)\n",
    "            output = decoder(features_7x7, features_14x14, features_28x28, features_56x56)\n",
    "\n",
    "            # Compute validation loss\n",
    "            loss = criterion(output, ab)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_bar.set_postfix(loss=f\"{val_loss/(i+1):.4f}\")\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(decoder.state_dict(), 'model_1.pth')\n",
    "        print(f\"Model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "VuS-OiImQhNh"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH7J21ViEW00"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2x0ynNOEW00",
    "outputId": "c8cfa0aa-85a8-44bf-ed44-ab84548456f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder().to(device)\n",
    "decoder.load_state_dict(torch.load('model_1.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHdW9qXQ7R6Z"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "P6SynNupcP_h"
   },
   "outputs": [],
   "source": [
    "# Get a batch from the test loader\n",
    "dataiter = iter(test_loader)\n",
    "L_batch, ab_batch = next(dataiter)\n",
    "L_batch, ab_batch = next(dataiter)\n",
    "L_batch, ab_batch = L_batch.to(device), ab_batch.to(device)\n",
    "L_batch = L_batch.repeat(1, 3, 1, 1)\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    features_56x56, features_28x28, features_14x14, features_7x7 = encoder(L_batch)\n",
    "\n",
    "    predicted_ab = decoder(features_7x7, features_14x14, features_28x28, features_56x56)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "eYBqgJftcUTx"
   },
   "outputs": [],
   "source": [
    "L_batch = L_batch[:, 0, :, :]\n",
    "L_batch = L_batch.unsqueeze(1)\n",
    "\n",
    "L_batch = (L_batch + 1) * 0.5 * 100\n",
    "predicted_ab = ((predicted_ab + 1) * 0.5 * (127 + 128)) - 128\n",
    "ab_batch = ((ab_batch + 1) * 0.5 * (127 + 128)) - 128\n",
    "\n",
    "# Combine L and ab channels\n",
    "predicted_lab = torch.cat([L_batch, predicted_ab], dim=1)\n",
    "real_lab = torch.cat([L_batch, ab_batch], dim=1)\n",
    "\n",
    "\n",
    "predicted_lab = predicted_lab.cpu().numpy()\n",
    "real_lab = real_lab.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L min/max: 50.0 5041.14990234375\n",
      "ab_real min/max: -21190.23046875 -13302.21875\n",
      "ab_pred min/max: -13631.90234375 -11854.4765625\n"
     ]
    }
   ],
   "source": [
    "print(\"L min/max:\", L.min().item(), L.max().item())\n",
    "print(\"ab_real min/max:\", ab_real.min().item(), ab_real.max().item())\n",
    "print(\"ab_pred min/max:\", ab_pred.min().item(), ab_pred.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "real_rgb = to_rgb_safe(L, ab_real)\n",
    "plt.imshow(real_rgb)\n",
    "plt.title(\"Ground Truth Check\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Manual dummy LAB: mid-gray with blue tint\n",
    "lab = np.zeros((224, 224, 3), dtype=np.float32)\n",
    "lab[:, :, 0] = 50         # L: mid-brightness\n",
    "lab[:, :, 1] = 0          # a: neutral\n",
    "lab[:, :, 2] = -50        # b: blue tint\n",
    "\n",
    "rgb = lab2rgb(lab)\n",
    "\n",
    "plt.imshow(rgb)\n",
    "plt.title(\"Manual LAB to RGB Test\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2gHPLZMRFDKd",
    "outputId": "30e5281b-8dfb-42dd-b3bc-8842be941246"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.color import lab2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def to_rgb_safe(L_tensor, ab_tensor):\n",
    "    # Remove batch dim\n",
    "    L = L_tensor.squeeze(0).cpu().numpy()\n",
    "    ab = ab_tensor.squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Denormalize properly\n",
    "    L = (L + 1) * 50                # [0, 100]\n",
    "    ab = ab * 127.5                 # [-128, 127]\n",
    "\n",
    "    # Stack to (H, W, 3)\n",
    "    lab = np.zeros((224, 224, 3), dtype=np.float32)\n",
    "    lab[:, :, 0] = L[0]            # L channel\n",
    "    lab[:, :, 1] = ab[0]           # a channel\n",
    "    lab[:, :, 2] = ab[1]           # b channel\n",
    "\n",
    "    # LAB to RGB\n",
    "    rgb = lab2rgb(lab)\n",
    "    return rgb\n",
    "\n",
    "# Choose a sample\n",
    "i = 0\n",
    "L = L_batch[i].unsqueeze(0)\n",
    "ab_real = ab_batch[i].unsqueeze(0)\n",
    "ab_pred = predicted_ab[i].unsqueeze(0)\n",
    "\n",
    "# Convert\n",
    "real_rgb = to_rgb_safe(L, ab_real)\n",
    "pred_rgb = to_rgb_safe(L, ab_pred)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(real_rgb)\n",
    "plt.title(\"Real Color Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(pred_rgb)\n",
    "plt.title(\"Predicted Color Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_blYq4v7ohb"
   },
   "source": [
    "## Evaulting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_XlyLCqjYNj",
    "outputId": "fd280005-9ae4-4401-a0ad-d38be8296b31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:30<00:00,  8.32it/s]\n"
     ]
    }
   ],
   "source": [
    "def prediction(model, test_loader):\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    original_images = []\n",
    "    predicted_images = []\n",
    "\n",
    "    for L_batch, ab_batch in tqdm(test_loader):\n",
    "        L_batch, ab_batch = L_batch.to(device), ab_batch.to(device)\n",
    "        input = L_batch.repeat(1, 3, 1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features_56x56, features_28x28, features_14x14, features_7x7 = encoder(input)\n",
    "\n",
    "            predicted_ab = model(features_7x7, features_14x14, features_28x28, features_56x56)            \n",
    "\n",
    "        L_batch = (L_batch + 1) * 0.5 * 100        \n",
    "        predicted_ab = ((predicted_ab + 1) * 0.5 * (127 + 128)) - 128\n",
    "        ab_batch = ((ab_batch + 1) * 0.5 * (127 + 128)) - 128\n",
    "\n",
    "        # Combine L and ab channels\n",
    "        predicted_lab = torch.cat([L_batch, predicted_ab], dim=1)\n",
    "        actual_lab = torch.cat([L_batch, ab_batch], dim=1)\n",
    "\n",
    "        predicted_lab = predicted_lab.cpu().numpy()\n",
    "        actual_lab = actual_lab.cpu().numpy()\n",
    "\n",
    "        predicted_images.extend(predicted_lab)\n",
    "        original_images.extend(actual_lab)\n",
    "\n",
    "    return original_images, predicted_images\n",
    "\n",
    "original_images, predicted_images = prediction(decoder, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4wxztv2mDFI",
    "outputId": "1f6739ac-2930-4315-93c3-44779a57443d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM: 0.9400\n",
      "Average PSNR: 29.9230\n"
     ]
    }
   ],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.color import lab2rgb\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(original_images, predicted_images):\n",
    "\n",
    "    total_ssim = 0.0\n",
    "    total_psnr = 0.0\n",
    "    total_samples = 0\n",
    "    for original_img, predicted_img in zip(original_images, predicted_images):\n",
    "        original_img = lab2rgb(original_img.transpose(1, 2, 0))\n",
    "        predicted_img = lab2rgb(predicted_img.transpose(1, 2, 0))\n",
    "\n",
    "        ssim_value = ssim(original_img, predicted_img, multichannel=True, channel_axis=2, data_range=1.0)\n",
    "        psnr_value = psnr(original_img, predicted_img, data_range=1.0)\n",
    "        total_ssim += ssim_value\n",
    "        total_psnr += psnr_value\n",
    "        total_samples += 1\n",
    "\n",
    "    average_ssim = total_ssim / total_samples\n",
    "    average_psnr = total_psnr / total_samples\n",
    "\n",
    "    return average_ssim, average_psnr\n",
    "\n",
    "ssim_value, psnr_value = evaluate_model(original_images, predicted_images)\n",
    "print(f\"Average SSIM: {ssim_value:.4f}\")\n",
    "print(f\"Average PSNR: {psnr_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCnnUQv2EW00"
   },
   "source": [
    "# Results\n",
    "\n",
    "We are getting quite good results but after observing carefully we saw that the model is learning green color more than the other colors."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
